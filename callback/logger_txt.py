import os
import logging
from lightning.pytorch.callbacks import Callback
from lightning.pytorch.utilities.rank_zero import rank_zero_only

def get_logger(log_file: str = "log.txt"):
    logger = logging.getLogger("MyLogger")
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        @rank_zero_only
        def init_loger():
            # æ§åˆ¶å°è¾“å‡ºï¼ˆå¯é€‰ï¼‰
            # console_handler = logging.StreamHandler()
            # console_handler.setLevel(logging.INFO)

            # æ–‡ä»¶è¾“å‡º
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(logging.INFO)

            # æ ¼å¼
            formatter = logging.Formatter("[%(asctime)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
            # console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)

            # logger.addHandler(console_handler)
            logger.addHandler(file_handler)
            
        init_loger()
    return logger

class logger_txt_callback(Callback):
    """
    ç”¨äºä¿å­˜è®­ç»ƒtxtæ—¥å¿—çš„å›è°ƒç±»ã€‚
    """
    def __init__(self):
         pass

    def on_fit_start(self, trainer, pl_module):
        # åªåœ¨è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–æ—¥å¿—å™¨
        log_dir = pl_module.logger.log_dir
        if log_dir is None:
            log_dir = "./none_logs"
        log_path = os.path.join(log_dir, "train_log.txt")
        pl_module.logger_txt = get_logger(log_path)
        pl_module.logger_txt.info("ğŸš€ Training started")

        pl_module.writer = pl_module.logger.experiment

    def on_train_epoch_end(self,trainer, pl_module):
        # è®­ç»ƒç»“æŸæ—¶è®°å½•å½“å‰epochçš„è®­ç»ƒæŸå¤±
        loss = trainer.callback_metrics.get("train_loss_epoch")
        if loss is not None:
            pl_module.logger_txt.info(f"ğŸ“£ Epoch {pl_module.current_epoch} finished. Train Loss: {loss:.4f}")