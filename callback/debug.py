from lightning.pytorch.callbacks import Callback

class debugger(Callback):
    """
    ç”¨äºè°ƒè¯•çš„å›è°ƒç±»ã€‚
    """
    def __init__(self):
        pass

    def on_after_backward(self, trainer, pl_module):
        return
        pl_module.logger_txt.info("ğŸ” [Debug] Checking unused parameters (after backward):")
        for name, param in pl_module.named_parameters():
            if param.requires_grad and param.grad is None:
                pl_module.logger_txt.info(f"âš ï¸ Unused parameter: {name}")

    def on_before_optimizer_step(self, trainer, pl_module, optimizer):
        return
        #DEBUGæ˜¯å¦æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸
        # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯ä¸€å±‚çš„æ¢¯åº¦èŒƒæ•°
        norms = {}

        # éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        for name, param in pl_module.named_parameters():
            if param.grad is not None:
                # è®¡ç®—å‚æ•°çš„ L2 èŒƒæ•°ï¼ˆå³ 2-èŒƒæ•°ï¼‰
                norm = param.grad.norm(2)
                # å°†èŒƒæ•°å­˜å‚¨åœ¨å­—å…¸ä¸­ï¼Œé”®ä¸ºå‚æ•°åç§°
                norms[f'grad_norm_{name}'] = norm

        # ä½¿ç”¨ log_dict è®°å½•æ‰€æœ‰æ¢¯åº¦èŒƒæ•°
        pl_module.log_dict(norms, prog_bar=True, logger=True)